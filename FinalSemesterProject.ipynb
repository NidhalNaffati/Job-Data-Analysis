{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ctjr3x6y38NZ"
   },
   "source": [
    "# Prepare our Json files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zY1VMYGrzps0"
   },
   "source": [
    "**Installing scrapy library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0fxn_om4Gh1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIekLr7e4G3e",
    "outputId": "a7d9f090-ff22-48d4-da39-d3ce508ebf8a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile jobs_1.py\n",
    "\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class JobsSpider(scrapy.Spider):\n",
    "    # Spider name\n",
    "    name = \"jobs\"\n",
    "\n",
    "    # List of starting URLs\n",
    "    start_urls = [\n",
    "        'https://tunisia.tanqeeb.com/s/jobs/jobs-in-tunisia'\n",
    "    ]\n",
    "\n",
    "    # Custom settings for the spider\n",
    "    custom_settings = {\n",
    "        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "        'COOKIES_ENABLED': True,\n",
    "        'DOWNLOAD_DELAY': 1  # Set the delay to 1 second (or adjust as needed)\n",
    "    }\n",
    "\n",
    "    # Initialize the page count\n",
    "    page_count = 0\n",
    "    \n",
    "    # Maximum number of pages to scrape\n",
    "    max_pages = 50\n",
    "\n",
    "    # Callback function that processes the response\n",
    "    def parse(self, response):\n",
    "        # Check if the response status is 403 (Forbidden)\n",
    "        if response.status == 403:\n",
    "            self.logger.warning(\"Received a 403 Forbidden response. You may be blocked. Check the website's terms of service.\")\n",
    "            return\n",
    "\n",
    "        # Extract job information from each 'div.card-body' element\n",
    "        for job in response.css('div.card-body'):\n",
    "            yield {\n",
    "                'location': job.css('p.h10 > span:nth-child(1)::text').get(),\n",
    "                'jobTitle': job.css('h5::text').get(),\n",
    "                'company': job.css('p.h10 > span:nth-child(2)::text').get(),\n",
    "                'Add Date': job.css('p.h10 > span:nth-child(3)::text').get(),\n",
    "            }\n",
    "        \n",
    "        # Increment the page count\n",
    "        self.page_count += 1\n",
    "\n",
    "        # Check if we have scraped 300 pages, and if not, follow pagination links\n",
    "        if self.page_count < self.max_pages:\n",
    "            yield from response.follow_all(css='li.page-item.active > a', callback=self.parse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOcNowkrzcMN",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if the file exists before deleting it\n",
    "if os.path.exists(\"jobs_1.json\"):\n",
    "    os.remove(\"jobs_1.json\")\n",
    "\n",
    "# Run your Scrapy spider\n",
    "# this might take some time and a long output\n",
    "!scrapy runspider jobs_1.py -o jobs_1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7m-bvhEqz7PI",
    "outputId": "c2e47f84-61d6-4304-d2fa-22a75b57aae4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile jobs_2.py\n",
    "\n",
    "import scrapy\n",
    "\n",
    "# Define a Scrapy spider class named 'jobsSpider'\n",
    "class jobsSpider(scrapy.Spider):\n",
    "    # Set the name of the spider\n",
    "    name = \"jobs\"\n",
    "\n",
    "    # Define the starting URLs for the spider\n",
    "    start_urls = [\n",
    "        \"https://www.farojob.net/jobs\"\n",
    "    ]\n",
    "\n",
    "    # Initialize a page counter\n",
    "    page_counter = 0\n",
    "\n",
    "    # Maximum number of pages to scrape\n",
    "    max_pages = 50\n",
    "\n",
    "    # Callback function to process the web page's response\n",
    "    def parse(self, response):\n",
    "        # Check if the response status is 403 (Forbidden)\n",
    "        if response.status == 403:\n",
    "            self.logger.warning(\"Received a 403 Forbidden response. You may be blocked. Check the website's terms of service.\")\n",
    "            return\n",
    "            \n",
    "            \n",
    "        # Increment the page counter\n",
    "        self.page_counter += 1\n",
    "\n",
    "        # Extract job information from each 'article.loadmore-item' element\n",
    "        for job in response.css('article.loadmore-item'):\n",
    "            yield {\n",
    "                'location': job.css('div.loop-item-content > p > span.job-location > a > em::text').get(),\n",
    "                'jobTitle': job.css(' div.loop-item-content > h2 > a::text').get(),\n",
    "                'company': job.css('div.loop-item-content > p > span.job-company > a::text').get(),\n",
    "                'Add Date': job.css('div.loop-item-content > p > span.job-date > time > span::text').get(),\n",
    "            }\n",
    "        \n",
    "        # Follow pagination links to continue scraping other pages, but stop if we've reached the maximum number of pages\n",
    "        if self.page_counter < self.max_pages:\n",
    "            yield from response.follow_all(css='div.pagination.list-center > a.next.page-numbers', callback=self.parse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UghJdkl0I8m",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if the file exists before deleting it\n",
    "if os.path.exists(\"jobs_2.json\"):\n",
    "    os.remove(\"jobs_2.json\")\n",
    "\n",
    "# Run your Scrapy spider\n",
    "!scrapy runspider jobs_2.py -o jobs_2.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UTKmndYnEYX"
   },
   "source": [
    "# Preparing our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "id": "J16z9eGq0Xfb",
    "outputId": "ec204f20-60a2-4d65-9f40-772b0d283259",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a data cleaning function\n",
    "def clean_data(df):\n",
    "    if df is not None:\n",
    "        df['Add Date'] = df['Add Date'].str.replace('\\n', '')\n",
    "        df['company'] = df['company'].str.replace('\\n', '')\n",
    "        df['jobTitle'] = df['jobTitle'].str.replace('\\n', '')\n",
    "        df['location'] = df['location'].str.replace('\\n', '')\n",
    "    return df\n",
    "\n",
    "# Attempt to read the JSON file\n",
    "try:\n",
    "    df1 = pd.read_json(\"jobs_1.json\")\n",
    "    df1 = clean_data(df1)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error reading jobs_1.json:\", e)\n",
    "    df1 = None\n",
    "\n",
    "# Attempt to read the JSON file\n",
    "try:\n",
    "    df2 = pd.read_json(\"jobs_2.json\")\n",
    "    df2 = clean_data(df2)\n",
    "except Exception as e:\n",
    "    print(\"Error reading jobs_2.json:\", e)\n",
    "    df2 = None\n",
    "\n",
    "\n",
    "\n",
    "# Check if dataframes were successfully loaded\n",
    "if df1 is not None and df2 is not None:\n",
    "    MyDataFrame = pd.concat([df1, df2], ignore_index=True, sort=False, axis=0)\n",
    "    display(MyDataFrame)\n",
    "    print(\"****** Information about our DataFrame ******* \")\n",
    "    print(df1.info())\n",
    "    print(type(MyDataFrame))\n",
    "else:\n",
    "    print(\"Dataframes could not be loaded due to JSON errors.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlv1BUG1nE6j"
   },
   "source": [
    "# Creating a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQxG-lJ801cG",
    "outputId": "05f53621-d617-42dc-e5ef-38ee8dff5f3a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store job titles\n",
    "corpus = []\n",
    "\n",
    "# Iterate through the 'jobTitle' column of MyDataFrame\n",
    "for i in MyDataFrame['jobTitle']:\n",
    "    # Check if the value is not None\n",
    "    if i is not None:\n",
    "        # Append the non-None job title to the corpus list\n",
    "        corpus.append(i)\n",
    "\n",
    "# Print the corpus (list of job titles)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjmObbddnFs5"
   },
   "source": [
    "# Remove Noise Function For our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUqLrndn5KKA",
    "outputId": "cc4acd0b-74fa-4963-9542-fc64d7713d96",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries from nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Download required resources (punkt tokenizer and stopwords data)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Create a list of stopwords for both English and French languages\n",
    "final_stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
    "\n",
    "# Define a function to remove noise from text\n",
    "def remove_noise(text, stop_words=final_stopwords_list):\n",
    "    # Tokenize the input text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        # Remove any non-alphanumeric characters\n",
    "        token = re.sub('[^A-Za-z0-9]+', '', token)\n",
    "        if len(token) > 1 and token.lower() not in stop_words:\n",
    "            # Convert the token to lowercase and add it to the cleaned tokens list\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    \n",
    "    # Join the cleaned tokens into a single string\n",
    "    cleaned_tokens = ' '.join(cleaned_tokens)\n",
    "    return cleaned_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gB2L_4wEnFRI"
   },
   "source": [
    "# Creating tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1yhNrFWZ41wE",
    "outputId": "7927d197-761b-448d-a518-dadde46a605b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import the TfidfVectorizer class from scikit-learn's feature_extraction.text module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object\n",
    "# - TfidfVectorizer is a tool for converting a collection of raw documents (in this case, job titles) to a matrix of TF-IDF features.\n",
    "# - The 'tokenizer' parameter is set to 'remove_noise', which is a custom tokenization function that preprocesses the text data.\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=remove_noise)\n",
    "\n",
    "# Use the .fit_transform() method of the TfidfVectorizer\n",
    "# - The .fit_transform() method processes the 'corpus' data (a list of job titles) and converts it into a TF-IDF matrix.\n",
    "# - TF-IDF stands for Term Frequency-Inverse Document Frequency, which is a numerical statistic used in natural language processing.\n",
    "# - It represents the importance of each word in relation to the entire collection of job titles.\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "# - The TF-IDF matrix contains numerical values that represent the importance of words in each job title.\n",
    "# - Each row corresponds to a job title, and each column corresponds to a unique word in the entire collection.\n",
    "# - The values in the matrix are the TF-IDF scores for each word in each job title.\n",
    "print(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWsY0FSPnGCm"
   },
   "source": [
    "# Clustering with Kmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nw11pR-GiarF",
    "outputId": "ff743092-ac27-4e50-8ec5-1a81237362f3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import the KMeans clustering algorithm from scikit-learn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize KMeans with 3 centroids (clusters)\n",
    "kmeans = KMeans(n_clusters=3, random_state=47)\n",
    "# 'n_clusters' specifies the number of clusters to create.\n",
    "# 'random_state' is used to ensure reproducibility of results.\n",
    "\n",
    "# Fit the KMeans model to the TF-IDF matrix\n",
    "kmeans.fit(tfidf_matrix)\n",
    "# This step applies the KMeans clustering algorithm to the TF-IDF matrix created earlier.\n",
    "# It finds cluster assignments for each data point (job title).\n",
    "\n",
    "# Store the cluster labels in a variable\n",
    "clusters = kmeans.labels_\n",
    "# After fitting the model, 'kmeans.labels_' contains the cluster assignment for each job title.\n",
    "\n",
    "# Print the cluster labels\n",
    "print(clusters)\n",
    "\n",
    "# Create a DataFrame to store job titles and their cluster labels\n",
    "MyDataFrame = pd.DataFrame(data=corpus, columns=['Job Title'])\n",
    "# 'corpus' contains the job titles, and we create a DataFrame column named 'Job Title' to store them.\n",
    "\n",
    "# Add the cluster labels to the DataFrame\n",
    "MyDataFrame[\"Labels\"] = clusters\n",
    "# We add a new column 'Labels' to the DataFrame to store the cluster assignments.\n",
    "\n",
    "# Print information about the DataFrame\n",
    "print(MyDataFrame.info())\n",
    "# This line prints basic information about the DataFrame, such as column names and data types.\n",
    "\n",
    "# Sort the DataFrame based on the 'Labels' column\n",
    "MyDataFrame = MyDataFrame.sort_values('Labels')\n",
    "# This step sorts the DataFrame based on the cluster labels.\n",
    "# It groups job titles by their assigned clusters.\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "print(MyDataFrame)\n",
    "\n",
    "# Print the type of the DataFrame\n",
    "print(type(MyDataFrame))\n",
    "# This line prints the type of the DataFrame, which should be 'pandas.core.frame.DataFrame'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rn3FcKWQnGc4"
   },
   "source": [
    "# Trying to visiualise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ncHpeg-LisBE",
    "outputId": "069e2c0c-d218-4655-daab-44f4671502a3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for data visualization and dimensionality reduction\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize PCA (Principal Component Analysis) with 2 components\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "# PCA is used to reduce the dimensionality of the data while preserving important information.\n",
    "# In this case, it reduces the TF-IDF matrix to 2 dimensions.\n",
    "\n",
    "# Fit PCA to the TF-IDF matrix and store the reduced vectors in 'pca_vecs'\n",
    "pca_vecs = pca.fit_transform(tfidf_matrix.toarray())\n",
    "# The 'toarray()' method converts the sparse TF-IDF matrix to a dense NumPy array.\n",
    "\n",
    "# Print the reduced PCA vectors\n",
    "print(\"********************************\\n\", pca_vecs)\n",
    "\n",
    "# Extract the first and second dimensions from the reduced vectors\n",
    "x0 = pca_vecs[:, 0]\n",
    "x1 = pca_vecs[:, 1]\n",
    "\n",
    "# Add cluster assignments and PCA vectors to the DataFrame\n",
    "MyDataFrame['cluster'] = clusters\n",
    "MyDataFrame['x0'] = x0\n",
    "MyDataFrame['x1'] = x1\n",
    "\n",
    "# Print the DataFrame with cluster assignments and PCA vectors\n",
    "print(MyDataFrame)\n",
    "\n",
    "# Define a custom color palette for the clusters\n",
    "knn_palette = sns.color_palette(['#000C1F', '#29757A', '#FF5050'])\n",
    "# This defines a color palette with three distinct colors for the clusters.\n",
    "\n",
    "# Create a scatterplot to visualize the data\n",
    "plt.figure(figsize=(9, 9))\n",
    "sns.set()\n",
    "sns.scatterplot(x='x0', y='x1',\n",
    "                data=MyDataFrame,\n",
    "                hue='cluster',\n",
    "                palette=knn_palette,\n",
    "                # Define markers for each cluster ('o', '^', 's')\n",
    "                markers=[',', '^', 'P'],\n",
    "                # Specify that the style parameter is based on the 'cluster' column\n",
    "                style='cluster',\n",
    "                # Increase the size of the points\n",
    "                s=100,\n",
    "                # Show the legend\n",
    "                legend=True\n",
    "                )\n",
    "plt.show()\n",
    "# This code creates a scatterplot to visualize the data in two dimensions after PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvbj-V_ylmXC"
   },
   "source": [
    "# Creating a spark context and reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UBbjTDAlDbN-",
    "outputId": "b5e14953-4c49-41e7-a72b-498f136ddefc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSPbLODHFWdV",
    "outputId": "d27682f1-dc76-4c77-8341-4951744dfa2f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for working with PySpark and Pandas\n",
    "import pandas as pd\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "# Create or retrieve a SparkContext (sc) instance\n",
    "sc = SparkContext.getOrCreate()\n",
    "# SparkContext is the entry point for Spark functionality. It manages the resources for your Spark application.\n",
    "\n",
    "# Create a SparkSession (spark) using the SparkContext\n",
    "spark = SparkSession(sc)\n",
    "# SparkSession provides a single entry point to interact with Spark functionality, including creating DataFrames.\n",
    "\n",
    "# Read JSON data from 'jobs_1.json' and 'jobs_2.json' into Pandas DataFrames\n",
    "df1 = pd.read_json(\"jobs_1.json\")\n",
    "df2 = pd.read_json(\"jobs_2.json\")\n",
    "\n",
    "# Concatenate the two Pandas DataFrames vertically (along rows) and ignore index\n",
    "# This combines the data from 'jobs_1.json' and 'jobs_2.json' into a single DataFrame\n",
    "DataFrame = pd.concat([df1, df2], ignore_index=True, sort=True, axis=0)\n",
    "\n",
    "# Create a PySpark DataFrame ('jobs_dataFrame') from the 'jobTitle' column of 'DataFrame'\n",
    "jobs_dataFrame = spark.createDataFrame(DataFrame['jobTitle'].to_frame())\n",
    "# This creates a PySpark DataFrame with a single column named 'jobTitle' from the 'DataFrame' Pandas DataFrame.\n",
    "\n",
    "# Print the type of 'jobs_dataFrame'\n",
    "print(type(jobs_dataFrame))\n",
    "# This line prints the type of the 'jobs_dataFrame' variable, which should be a PySpark DataFrame.\n",
    "\n",
    "# Show the first 15 rows of the 'jobs_dataFrame' DataFrame without truncating column data\n",
    "jobs_dataFrame.show(n=15, truncate=False)\n",
    "# This displays the first 15 rows of the 'jobs_dataFrame' DataFrame without truncating the column data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxPzqgAIlmhx"
   },
   "source": [
    "# Creating a pipeline nd Clustering Using Kmean Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3D5xJEV6OMxU",
    "outputId": "6e0d8d16-8a90-4065-c05b-3c96415edf6f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules from PySpark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Download stopwords data using NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Create a Tokenizer transformer that converts 'jobTitle' text to lowercase and splits it by white spaces\n",
    "tokenizer = Tokenizer(inputCol=\"jobTitle\", outputCol=\"tokens\")\n",
    "\n",
    "# Remove rows with null values from 'jobs_dataFrame'\n",
    "df = jobs_dataFrame.dropna()\n",
    "\n",
    "# Create a list of stopwords for both English and French languages\n",
    "final_stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
    "\n",
    "# Create a StopWordsRemover transformer to remove stopwords from 'tokens' column\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"stopWordsRemovedTokens\", stopWords=final_stopwords_list)\n",
    "\n",
    "# Calculate Term Frequency (TF) using HashingTF with a specified number of features (numFeatures)\n",
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\", numFeatures=200)\n",
    "\n",
    "# Calculate Inverse Document Frequency (IDF)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)\n",
    "\n",
    "# Apply the K-Means clustering algorithm with k=3 (3 clusters)\n",
    "kmeans = KMeans(k=3)\n",
    "\n",
    "# Create a Pipeline with a sequence of stages for data processing and modeling\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, kmeans])\n",
    "\n",
    "# Fit the pipeline model to the DataFrame 'df'\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "# Transform the DataFrame 'df' using the trained pipeline model\n",
    "results = model.transform(df)\n",
    "\n",
    "# Display the results (show the DataFrame)\n",
    "display(results)\n",
    "\n",
    "# Print the first 25 rows of the results DataFrame\n",
    "results.show(25)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9pqbtqySnB5O",
    "ZiQj6nB0ln9B"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
